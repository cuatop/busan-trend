import requests
import json
import datetime
import re
import os
from collections import Counter
import urllib.parse

# === 1. ê²€ìƒ‰ì–´ ê³ ê¸‰í™” (ê²€ìƒ‰ì˜ ì§ˆì„ ë†’ì„) ===
# ë‹¨ìˆœíˆ "ë¶€ì‚° ì—¬í–‰"ì´ ì•„ë‹ˆë¼, ì•Œì§œë°°ê¸°ê°€ ë‚˜ì˜¬ë§Œí•œ ê²€ìƒ‰ì–´ë¡œ ë³€ê²½
SEARCH_KEYWORDS = [
    "ë¶€ì‚° í˜„ì§€ì¸ ë§›ì§‘", "ë¶€ì‚° ë…¸í¬ ë§›ì§‘", "ë¶€ì‚° ë¯¸ì‰ë¦°", "ë¶€ì‚° ë¸”ë£¨ë¦¬ë³¸",
    "ë¶€ì‚° ì˜¤ì…˜ë·° ì¹´í˜", "ë¶€ì‚° ìˆ¨ì€ ëª…ì†Œ", "ë¶€ì‚° í•«í”Œ ì†”ì§í›„ê¸°", 
    "ë¶€ì‚° ê¸°ì¥ ë§›ì§‘", "ë¶€ì‚° ì˜ë„ ë§›ì§‘", "ë¶€ì‚° ê´‘ì•ˆë¦¬ ì°ë§›ì§‘"
]
MAX_RESULTS = 50

API_KEY = os.environ.get('YOUTUBE_API_KEY')

def clean_korean_text(text):
    # 1. ìœ íŠœë¸Œ ì „ìš© ì¡ë‹¤í•œ ìš©ì–´ ì‚¬ì „ ì œê±° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    text_lower = text.lower()
    spam_terms = [
        "shorts", "vlog", "eng", "sub", "feat", "ep", "4k", "fhd", "hd", 
        "ë¸Œì´ë¡œê·¸", "ìë§‰", "ì§ìº ", "í‹°ì €", "ê³µì‹", "í•˜ì´ë¼ì´íŠ¸", "í’€ë²„ì „", 
        "ê´‘ê³ ", "í˜‘ì°¬", "í¬í•¨", "ë¬¸ì˜", "êµ¬ë…", "ì¢‹ì•„ìš”"
    ]
    for spam in spam_terms:
        if spam in text_lower:
            return [] # ê´‘ê³ ë‚˜ ì¡ë‹¤í•œ ì˜ìƒì€ ì•„ì˜ˆ ë¶„ì„ í¬ê¸°

    # 2. íŠ¹ìˆ˜ë¬¸ì ë° ìˆ«ì ì œê±° (ìˆ«ìê°€ ì„ì¸ ì¡ë‹¤í•œ ìˆœìœ„ ì œê±°: Top10, 3ê°€ì§€ ë“±)
    # ìˆœìˆ˜ í•œê¸€ê³¼ ì˜ì–´ë§Œ ë‚¨ê¹€
    text = re.sub(r'[0-9]+', '', text) 
    text = re.sub(r'[^\w\sê°€-í£a-zA-Z]', ' ', text)
    
    words = text.split()
    cleaned_words = []
    
    # 3. [í•µì‹¬] í”„ë¦¬ë¯¸ì—„ ë¶ˆìš©ì–´ í•„í„° (ì¡ë‹¤í•œ ë‹¨ì–´ ê°•ë ¥ ì‚­ì œ)
    garbage = set([
        # ì§€ì—­/êµ­ê°€ (ë„ˆë¬´ ë„“ì€ ë²”ìœ„)
        "ë¶€ì‚°", "í•œêµ­", "korea", "busan", "japan", "ì¼ë³¸", "ì„œìš¸", "ì „êµ­", "ê²½ìƒë„",
        # ìœ íŠœë¸Œ/ì—¬í–‰ ìƒíˆ¬ì–´
        "ë§›ì§‘", "ì—¬í–‰", "ê´€ê´‘", "íˆ¬ì–´", "í›„ê¸°", "ë¦¬ë·°", "review", "trip", "travel", "food", 
        "mukbang", "ë¨¹ë°©", "ìŒì‹", "ì‹ë‹¹", "ì¹´í˜", "cafe", "street", "road", "view",
        # ë¬´ì˜ë¯¸í•œ í˜•ìš©ì‚¬/ë™ì‚¬/ë¶€ì‚¬
        "ì§„ì§œ", "ì •ë§", "ì™„ì „", "ëŒ€ë°•", "ì—­ëŒ€ê¸‰", "ìµœê³ ", "best", "top", "hot", "new",
        "ìœ ëª…í•œ", "ì†”ì§", "ì¶”ì²œ", "ê°•ì¶”", "ë¹„ë°€", "ìˆ¨ì€", "ë‚˜ë§Œ", "ì•Œê³ ì‹¶ì€", "ê³µê°œ",
        "ê°€ì„±ë¹„", "ì €ë ´í•œ", "ë¹„ì‹¼", "ì¡´ë§›", "ê¿€ë§›", "ë¯¸ì¹œ", "ê°œì©ŒëŠ”", "ë¬´ì¡°ê±´", "ì ˆëŒ€",
        "ê°€ì§€", "ê³³ì€", "ì—¬ê¸°", "ì €ê¸°", "ê±°ê¸°", "ì–´ë””", "ì˜¤ëŠ˜", "ì§€ê¸ˆ", "ê·¼í™©", "ì¼ìƒ",
        "í•˜ëŠ”", "ìˆëŠ”", "ê°€ëŠ”", "ì˜¤ëŠ”", "ë¨¹ëŠ”", "ë³´ëŠ”", "ê°€ë³¸", "ë¨¹ì–´ë³¸", "í•´ë³¸",
        "ê°€ì„¸ìš”", "ì˜¤ì„¸ìš”", "ë³´ì„¸ìš”", "ê°‘ë‹ˆë‹¤", "ì˜µë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ë©ë‹ˆë‹¤", "ì…ë‹ˆë‹¤",
        "ì‚¬ëŒ", "í˜„ì§€ì¸", "í† ë°•ì´", "ì™¸êµ­ì¸", "ì»¤í”Œ", "ê°€ì¡±", "í˜¼ì", "ë°ì´íŠ¸", "ì½”ìŠ¤",
        "ì‹œê°„", "ìœ„ì¹˜", "ê°€ê²©", "ì£¼ì°¨", "ì˜ˆì•½", "ì›¨ì´íŒ…", "ì •ë³´", "ê¿€íŒ", "ì´ì •ë¦¬", "ëª¨ìŒ",
        "ì‹¤íŒ¨", "ì„±ê³µ", "ì´ìœ ", "ì¶©ê²©", "ì‹¤í™”", "íŠ¹ì§‘", "í¸", "íƒ„", "ë¶€", "í˜¸",
        "ë‹¤ì‹œ", "ì§ì ‘", "ì„ ì •", "ë°œê²¬", "ì¶œì—°", "ë“±ì¥", "ì†Œê°œ", "ì •ë¦¬", "ë¹„êµ", "ë¶„ì„"
    ])
    
    # ì¡°ì‚¬ ì œê±° ë¦¬ìŠ¤íŠ¸
    suffixes = ["ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì—", "ì˜", "ì„œ", "ë¡œ", "ê³ ", "í•˜ê³ ", "ì—ì„œ", "ì´ë‘", "ê¹Œì§€", "ë¶€í„°", "ìœ¼ë¡œ", "ë„¤ìš”", "ì„¸ìš”", "ìš°ì™€", "ì¸ê°€", "ì¸ê°€ìš”", "ì…ë‹ˆë‹¤", "ìŠµë‹ˆë‹¤", "ì—ë„", "ì´ë‚˜"]

    for w in words:
        word_to_add = w
        
        # ì¡°ì‚¬ ì œê±°
        if len(word_to_add) > 1:
            for suffix in suffixes:
                if word_to_add.endswith(suffix):
                    word_to_add = word_to_add[:-len(suffix)]
                    break
        
        # [ìµœì¢… í•„í„°] 
        # 1. ê¸€ììˆ˜ 2ê¸€ì ì´ìƒ
        # 2. ë¶ˆìš©ì–´(garbage)ì— ì—†ì–´ì•¼ í•¨
        # 3. ì˜ì–´ì¸ ê²½ìš° ì†Œë¬¸ìë¡œ ë°”ê¿”ì„œ ë¶ˆìš©ì–´ ì²´í¬
        if len(word_to_add) >= 2 and word_to_add.lower() not in garbage:
            cleaned_words.append(word_to_add)
            
    return cleaned_words

def get_real_youtube_data():
    all_words = []
    
    if not API_KEY:
        return []

    print("ğŸš€ í”„ë¦¬ë¯¸ì—„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    for keyword in SEARCH_KEYWORDS:
        # viewCount(ì¡°íšŒìˆ˜) ìˆœìœ¼ë¡œ ê°€ì ¸ì™€ì„œ ê²€ì¦ëœ ê³³ ìœ„ì£¼ë¡œ
        url = f"https://www.googleapis.com/youtube/v3/search?part=snippet&q={keyword}&key={API_KEY}&maxResults={MAX_RESULTS}&type=video&order=viewCount"
        
        try:
            response = requests.get(url)
            data = response.json()
            
            if 'items' in data:
                for item in data['items']:
                    title = item['snippet']['title']
                    words = clean_korean_text(title)
                    all_words.extend(words)
        except: continue
            
    return Counter(all_words).most_common(80)

try:
    word_counts = get_real_youtube_data()
except:
    word_counts = []

d3_data = []
if word_counts:
    max_count = word_counts[0][1]
    for word, count in word_counts:
        # í´ë¦­ì‹œ 'ì†”ì§í›„ê¸°' ê²€ìƒ‰
        search_query = f"ë¶€ì‚° {word} ì†”ì§í›„ê¸°" 
        encoded_query = urllib.parse.quote(search_query)
        link = f"https://www.youtube.com/results?search_query={encoded_query}"
        
        size = 15 + (count / max_count) * 85
        d3_data.append({"text": word, "size": size, "url": link, "count": count})

    html_template = """
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Busan Premium Trends</title>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://cdn.jsdelivr.net/gh/holtzy/D3-graph-gallery@master/LIB/d3.layout.cloud.js"></script>
        <link href="https://fonts.googleapis.com/css2?family=Black+Han+Sans&family=Noto+Sans+KR:wght@400;700;900&display=swap" rel="stylesheet">
        <style>
            body { 
                margin: 0; padding: 0; 
                background-color: #e0f7fa; 
                text-align: center; 
                overflow: auto; 
                font-family: 'Noto Sans KR', sans-serif;
                display: flex; flex-direction: column; align-items: center; justify-content: center;
                min-height: 100vh;
            }
            #container { width: 100%; height: auto; display: flex; flex-direction: column; align-items: center; padding: 20px 0; }
            h2 { 
                color: #006064; margin: 10px 0 5px 0; 
                font-family: 'Black Han Sans', sans-serif; 
                font-size: 3em; 
                text-shadow: 2px 2px 0px #fff;
            }
            .footer { font-size: 0.9em; color: #555; margin-bottom: 20px; font-weight: bold; }
            .word-link { cursor: pointer; transition: all 0.2s ease; }
            .word-link:hover { opacity: 0.7 !important; text-shadow: 1px 1px 5px rgba(255,255,255,0.8); }
            
            #cloud-area { width: 100%; flex-grow: 1; display: flex; align-items: center; justify-content: center; }
            
            /* SVG ë°˜ì‘í˜• ì„¤ì • */
            svg { width: 95%; height: auto; max-width: 1200px; display: block; }
        </style>
    </head>
    <body>
        <div id="container">
            <h2>ğŸŒŠ ë¶€ì‚° í•«í”Œë ˆì´ìŠ¤ & ë§›ì§‘</h2>
            <p class="footer">Premium YouTube Analysis â€¢ Updated: __DATE_PLACEHOLDER__</p>
            <div id="cloud-area"></div>
        </div>

        <script>
            var words = __DATA_PLACEHOLDER__;
            var myColor = d3.scaleOrdinal().range(["#01579b", "#0288d1", "#00acc1", "#00bfa5", "#ff6f00", "#d84315", "#c2185b"]);

            var layoutWidth = 1000;
            var layoutHeight = 800; 

            var layout = d3.layout.cloud()
                .size([layoutWidth, layoutHeight])
                .words(words.map(function(d) { return {text: d.text, size: d.size, url: d.url, count: d.count}; }))
                .padding(5) 
                .rotate(function() { return (~~(Math.random() * 6) - 3) * 30; })
                .font("Noto Sans KR")
                .fontWeight("900")
                .fontSize(function(d) { return d.size; })
                .on("end", draw);

            layout.start();

            function draw(words) {
              d3.select("#cloud-area").append("svg")
                  .attr("viewBox", "0 0 " + layoutWidth + " " + layoutHeight)
                  .attr("preserveAspectRatio", "xMidYMid meet")
                .append("g")
                  .attr("transform", "translate(" + layoutWidth / 2 + "," + layoutHeight / 2 + ")")
                .selectAll("text")
                  .data(words)
                .enter().append("text")
                  .attr("class", "word-link")
                  .style("font-size", function(d) { return d.size + "px"; })
                  .style("font-family", "'Noto Sans KR', sans-serif")
                  .style("font-weight", "900")
                  .style("fill", function(d, i) { return myColor(i); })
                  .attr("text-anchor", "middle")
                  .attr("transform", function(d) {
                    return "translate(" + [d.x, d.y] + ")rotate(" + d.rotate + ")";
                  })
                  .text(function(d) { return d.text; })
                  .on("click", function(d) { window.open(d.url, '_blank'); })
                  .append("title")
                  .text(function(d) { return d.text + " (Click for Premium Info)"; });
            }
        </script>
    </body>
    </html>
    """

    json_str = json.dumps(d3_data)
    today_str = datetime.date.today().strftime('%Y-%m-%d')
    final_html = html_template.replace("__DATA_PLACEHOLDER__", json_str).replace("__DATE_PLACEHOLDER__", today_str)
    
    with open("index.html", "w", encoding="utf-8") as f:
        f.write(final_html)
else:
    with open("index.html", "w", encoding="utf-8") as f:
        f.write("<h2>No Data Found</h2>")
